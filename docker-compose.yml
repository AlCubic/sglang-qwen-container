services:
  sglang-server:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    container_name: sglang-qwen-inference
    hostname: sglang-inference
    
    # Конфигурация GPU (автоматически отключается для CPU режима)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    
    # Проброс портов
    ports:
      - "${SGLANG_PORT:-5000}:${SGLANG_PORT:-5000}"
      - "5001:5001"
    
    # Монтирование томов для persistent данных
    volumes:
      - ./models:/models
      - ./data:/data
      - ./logs:/logs
      - ./config/custom_sglang.json:/app/config/custom_sglang.json:ro
    
    # Переменные окружения
    environment:
      - HOME=/tmp
      - MODEL_NAME=Qwen/Qwen2.5-0.5B-Instruct
      - SGLANG_PORT=${SGLANG_PORT:-5000}
      - SGLANG_HOST=0.0.0.0
      - SGLANG_DTYPE=${SGLANG_DTYPE:-bfloat16}
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - NVIDIA_REQUIRE=cuda>=12.0
      - FLASHINFER_WORKSPACE_DIR=/tmp/flashinfer
      - TORCH_CUDA_ARCH_LIST=7.5
      # CPU Offload параметры (опционально)
      - SGLANG_ENABLE_CPU_OFFLOAD=${SGLANG_ENABLE_CPU_OFFLOAD:-false}
      - OFFLOAD_DIR=/tmp/offload
    
    # Конфигурация перезапуска
    restart: unless-stopped
    
    # Проверка здоровья контейнера
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${SGLANG_PORT:-5000}/health || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    
    # Логирование
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

# Определение сетей
# networks:
#   sglang-network:
#     driver: bridge
#     ipam:
#       config:
#         - subnet: 172.28.0.0/16
